\documentclass{beamer}
\usefonttheme[onlymath]{serif}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{cancel}
\usepackage[export]{adjustbox}
\usepackage[utf8]{inputenc}
\usepackage{tikz} 
\usetikzlibrary{bayesnet}
% definitions
\def\H{\mathcal{H}}
\def\X{\mathbf{X}}
\def\w{\mathbf{w}}
\def\W{\mathbf{W}}
\def\const{\mathrm{const}}
\def\Var{\mathrm{Var}}
\def\tr{\mathrm{tr}}
\def\T{\top}
\def\U{\mathbf{U}}
\def\S{\mathbf{S}}
\def\V{\mathbf{V}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\minimize}{\mathop{\mathrm{minimize}}}
\newcommand{\maximize}{\mathop{\mathrm{maximize}}}
\newcommand{\st}{\mathop{\mathrm{subject\,\,to}}}
\newcommand\mat[1]{\begin{bmatrix}#1\end{bmatrix}} 
%Information to be included in the title page:
\usecolortheme{seahorse}
\title{Latent Dirichlet Allocation}
\author{Kangcheng Hou\\kangchenghou@gmail.com}
\institute{Zhejiang University}
\date{\today}

    
    
\begin{document}
    
\frame{\titlepage}

\input{part1.tex} 

\begin{frame}[allowframebreaks]{Variational Inference}
Now let's see how to implement do variational inference for the following model.
\input{svi-pgm.tex}
The joint distribution is 
$$p(x,z,\beta | \alpha)=p(\beta | \alpha)\prod_{n=1}^N p(x_n,z_n | \beta)$$
We make the following assumptions, the conditional distribution belongs to exponential family distribution,
$$p(\beta | x,z,\alpha)=h(\beta)\exp(\eta_g (x,z,\alpha)^\top T(\beta)-A_g(\eta_g(x,z,\alpha)))$$
and
$$\scriptstyle p(z_{nj}|x_n,z_{n,\neg j},\beta)=h(z_{nj})\exp(\eta_l(x_n,z_{n,\neg j},\beta)^\top T(z_{nj})-A_l(\eta_l(x_n,z_{n,\neg j},\beta)))$$
The evidence lower bound is
$$\mathcal{L}(q)=\mathbb{E}_q[\log p(x,z,\beta)]-\mathbb{E}_q[\log q(z,\beta)]$$
We select the distribution $q$ such that it is both expressive and easier to optimize, (Note that we only restrict the function form for $p$, but have not specifies the factorization form of $p$.)
$$q(z,\beta)=q(\beta|\lambda)\prod_{n=1}^N \prod_{j=1}^J q(z_{nj}|\phi_{nj})$$
Other than the factorization of $q(z,\beta)$, we restrict $q(z, \beta)$ and $q(z_{nj} | \phi_{nj})$.
$$q(\beta | \lambda)=h(\beta)\exp(\lambda^\top T(\beta)-A_g(\lambda))$$
$$q(z_{nj}|\phi_{nj})=h(z_{nj})\exp(\phi_{nj}^\top T(z_{nj})-A_l(\phi_{nj}))$$
Now let's alter the parameter $\lambda, \phi$ to maximize the ELBO.
Rather than $\mathcal{L}(q)$, we can write it as $\mathcal{L}(\lambda, \phi)$.
It turns out we can optimize $\lambda$ and $\phi$ in turn.  We first fix $\phi$ and optimize $\lambda$. 

\begin{align*}
\mathcal{L}(q)& =\mathbb{E}_q[\log p(x,z,\beta)]-\mathbb{E}_q[\log q(z,\beta)] \\
& = \mathbb{E}_q[\log p(\beta | x,z)] + \underbrace{\mathbb{E}_q[\log p(x,z)]}_{\text{not related to }\beta}-\mathbb{E}_q[\log q(z,\beta)] \\
& = \mathbb{E}_q[\log h(\beta)] + \mathbb{E}_q[\eta_g (x,z)]^\top \mathbb{E}_q[T(\beta)]-\mathbb{E}_q[A_g(\eta_g(x,z))]  \\ 
& \quad  - \mathbb{E}_q[\log h(\beta)] - \mathbb{E}_q[T(\beta)^\top \lambda] +  \mathbb{E}[A_g(\lambda)] \\
& = \mathbb{E}_q[\eta_g (x,z)]^\top\mathbb{E}_q[T(\beta)]-\mathbb{E}_q[A_g(\eta_g(x,z))] - \mathbb{E}_q[T(\beta)^\top \lambda] \\ 
& \quad + \mathbb{E}[A_g(\lambda)] \\ 
& = \mathbb{E}_q[\eta_g (x,z)]^\top\mathbb{E}_q[T(\beta)] - \mathbb{E}_q[T(\beta)]^\top\lambda + A_g(\lambda) \\
& = \nabla_\lambda A_g(\lambda)^\top(\mathbb{E}_{q(z | \phi)}[\eta_g (x,z)] - \lambda) + A_g(\lambda)
\end{align*}
Maximize w.r.t $\mathcal{L}(\lambda)$ a.k.a $\mathcal{L}(q)$, $\frac{d\mathcal{L}(\lambda)}{d \lambda} = 0$, we have 
$$\nabla^2_\lambda A_g(\lambda)^\top(\mathbb{E}_{q(z | \phi)}[\eta_g (x,z)] - \lambda) = 0$$

\framebreak 
Assume that $\nabla^2_\lambda A_g(\lambda) \neq 0$, so we have
$$\mathbb{E}_{q(z | \phi)}[\eta_g (x,z)] - \lambda = 0$$
That's cool! Right? What have we done so far?
\begin{itemize}
\item We first specifies a graphical model, and specifies the latent variables e.g. $z, \beta$.
\item We define the distribution so that the conditional distribution for latent varaibels given other variables is always exponential family distribution.
\item For joint distribution of latent variable e.g. $q(z, \beta)$, we assume that $q(z, \beta)$ factorizes into $q(z | \lambda)q(\beta | \phi)$. 
\item Then, we have a very nice conclusion from our previous lengthy derivation that if we optimize the parameters e.g. $\lambda, \phi$ one by one, we just do it in this way. Repeat between $\lambda = \mathbb{E}_{q(z | \phi)}[\eta_g (x,z)]$ and $\phi = \mathbb{E}_{q(z | \lambda)}[\eta_l (x,\beta)]$ which is \textbf{set the parameter to be the expectation of natural prameter conditioning on all other latent variables}.
\end{itemize}

\end{frame}


\end{document}