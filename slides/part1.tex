\begin{frame}{Introduction}
    Here is a PGM of LDA.
    \input{lda.tex}
    We want to sample the posterior distribution of the latent variable $z_{d,i}$ given $w_{d,i}$. One method is gibbs sampling, in every iteration, we sample $z_i$ from distribution $p(z_i | z_{\neg i}, w)$.
    The joint distribution is 
    $$p(w,z | \alpha, \beta) = p(w | z, \beta) p(z | \alpha)$$
    \end{frame}
    \begin{frame}{Posterior predictive of Dirichlet-Multinomial}
    Suppose we have dirichlet prior distribution,
    $$p(\theta | \alpha) = \text{Dir}(\theta | \alpha) \propto \prod_{k=1}^K \theta_i^{\alpha_i - 1}$$
    
    \end{frame}
    \begin{frame}[allowframebreaks]{Joint distribution}
    The joint distribution is $p(w,z | \alpha, \beta) = p(w | z, \beta) p(z | \alpha)$. The first part is
    
    $$p(w | z, \beta) = \int_{\phi_{1 : K}} p(w | z, \phi_{1 : K})p(\phi_{1 : K} | \beta)d \phi_{1 :K}$$
    Let's see $p(w | z, \phi_{1 : K})$ first, 
    $$p(w | z, \phi_{1 : K}) = \prod_{i=1}^W \phi_{z_i, w_i}$$
    Or we can rephrase it in another way, where we classify it by topic.
    $$p(w | z, \phi_{1 : K}) = \prod_{i=1}^W \phi_{z_i, w_i} = \prod_{k=1}^K \prod_{v=1}^V \phi_{k,v}^{n_k^{(v)}}$$
    And $$p(\phi_{1 : K} | \beta) = \prod_{k=1}^K \prod_{v=1}^V \phi_{k,v}^{\beta_v - 1} $$
    Adding these two together, we have
    $$p(w | z, \beta) = \prod_{k=1}^K \frac{B(n_k + \beta)}{B(\beta)}$$
    $n_k$ represents the word apperance frequencies in topic $k$.
    The topic distribution $p(z | \alpha)$ can be derived similarly,
    $$p(z | \alpha) = \prod_{d=1}^D\frac{B(n_d + \alpha)}{B(\alpha)}$$
    So the conditional distribution is
    $$p(z,w | \alpha, \beta) = \prod_{k=1}^K \frac{B(n_k + \beta)}{B(\beta)}\prod_{d=1}^D\frac{B(n_d + \alpha)}{B(\alpha)}$$
    $$p(z_i = k | z_{\neg i}, w, \alpha, \beta) = \frac{p(w,z) | \alpha, \beta}{p(w, z_{\neg i} | \alpha, \beta)} \propto p(w,z | \alpha, \beta) $$
    Thus we just sample the $z_i$ according to the conditional distribution
    $$p(z_i = k | z_{\neg i}, w, \alpha, \beta) \propto \prod_{k=1}^K \frac{B(n_k + \beta)}{B(\beta)}\prod_{d=1}^D\frac{B(n_d + \alpha)}{B(\alpha)}$$
    We shall check out what is the same and what is different for different $k$ where $z_i = k$. As a result, it can be shown that 
    $$p(z_i = k | z_{\neg i}, w, \alpha, \beta) \propto \Gamma(n_{k, w_i}^{\neg i}) \Gamma(n_{d, k}^{\neg i})$$
    The symbol here is quite messy, if you have question, please feel free to contact me. $n_{k, w_i}^{\neg i}$ means except for this word, the frequency of word $w_i$ in topic $k$. And $n_{d, k}^{\neg i}$ means that except for this word, the frequency of word in document $d$ in topic $k$.
    This is \text{collapsed Gibbs sampling}. In implementing this, \textbf{what we need to do is keeps sampling} $z$, and that is enough.
    And after the sampling is done, the multinomial parameters can be derived as follows
    $$p(\theta_d | w, z, \alpha) \sim \text{Dir}(\theta_m | n_m + \alpha)$$
    $$p(\phi_d | w, z, \beta) \sim \text{Dir}(\phi_d | n_d + \beta) $$
    \end{frame}
    
    \begin{frame}[allowframebreaks]{Evidence Lower Bound(ELBO)}
    $$\ln (p(X)) = \ln (\frac{p(X,Z)}{q(Z)} - \ln (\frac{p(Z|X)}{q(Z)})$$
    Taking the expecttion w.r.t $q(Z)$ on both sides
    \begin{align*}
    \ln (P(X)) & = \int q(Z) \ln (\frac{p(X,Z)}{q(Z)} dZ - \int q(Z)\ln (\frac{p(Z|X)}{q(Z)}) dZ \\
    & = \mathcal{L}(q) + KL(q || p)
    \end{align*}
    Alternative derivation using Jensen's inequality:
    \begin{align*}
    \ln (X) & = \ln \int_Z p(X,Z) dZ \\
    & = \ln \int_Z p(X,Z) \frac{q(Z)}{q(Z)} dZ \\
    & = \ln \mathbb{E}_{Z \sim q(Z)} [\frac{p(X,Z)}{q(Z)}] \\
    & \ge \ln \mathbb{E}_{Z \sim q(Z)} [\frac{p(X,Z)}{q(Z)}] \\
    & = \int q(Z) \ln (\frac{p(X,Z)}{q(Z)}) dZ
    \end{align*}
    Which is $\mathcal{L}(q)$. We are interested in finding the lower bound of $\ln(X)$. And when the selected $q(Z)$ is closest to $p(Z|X)$, the KL divergence is 0, therefore, minimum is achieved.
    There are two typical approaches of get $q(Z)$,
    \begin{itemize}
    \item Assume that $q(Z)$ has some factorization $q(Z) = \prod_{i=1}^M q_i(Z_i)$.
    \item Assume $q(Z)$ is in some families of distribution(e.g. exponential families).
    \end{itemize}
    \end{frame}
    \begin{frame}{Factorization form}
    Assume that $q(z)$ can be factorized as follows, $q(z) = \sum_{i=1}^M q_i(z_i)$
    \begin{align*}
    \mathcal{L}(q) & = \int q(z) \ln p(x,z) dz - \int q(z) \ln q(z) dz \\
    & = \int \prod_{i=1}^M q_i(z_i) \ln p(x,z) dz - \int \prod_{i=1}^M q_i(z_i) \sum_{i=1}^M \ln q_i(z_i) dz \\
    \end{align*}
    If we just optimize $q_i(z_i)$ one at a time as follows,(Note that it is a iterative process)
    \begin{align*}
    \mathcal{L}(q) & = \int q_i(z_i)\mathbb{E}_{z_{\neg i}}[\ln p(x,z)]dz_i - \int q_i(z_i) \ln q_i(z_i) dz_i\\
    & = \int q_i(z_i)\ln \tilde{p}(x,z_i)dz_i - \int q_i(z_i) \ln q_i(z_i) dz_i \\
    & = \int q_i(z_i) \ln \frac{\tilde{p}(x,z_i)}{q_i(z_i)} = KL[\tilde{p}(x,z_i) || q_i(z_i)]
    \end{align*}
    \end{frame}
    
    \begin{frame}[allowframebreaks]{Exponential family distribution}
    Exponential family distribution is a rich family of distribution which computational tractable which is suitable for variational inference.
    $$p(x) = h(x) e^{\eta^\top T(x) - A(\eta)}$$
    Probability distribution in exponential family has several advantages. We can see that it is very easy to get the maximum likelihood estimation.
    
    $$\ln \prod_{i=1}^N p(x_i | \eta) = \sum_{i=1}^N \ln h(x_i) + \eta^\top (\sum_{i=1}^N T(x_i)) - N A(\eta)$$
    So if we want to get the maximum likelihood estimation,
    $$\frac{dA(\eta)}{d \eta} = \frac{\sum_{i=1}^N T(x_i)}{N}$$
    
    \framebreak
    
    Let's see an example of one-dimensional Gaussian distribution,
    $$\mathcal{N}(x | \mu, \sigma^2) = (2 \pi \sigma^2)^{-\frac{1}{2}} \exp(-\frac{(x - \mu)^2}{2 \sigma^2})$$
    We transform it into the exponential family distribution,
    $$p(x | \eta) = h(x) e^{\eta^\top T(x) - A(\eta)} = \exp(\mat{x \\ x^2}^\top \mat{\eta_1 \\ \eta_2} - (\frac{-\eta_1^2}{4\eta_2} + \frac{1}{2}\ln \eta_2) - \frac{1}{2} \ln 2\pi)$$
    $$\mat{\eta_1 \\ \eta_2} = \mat{\frac{\mu}{\sigma^2} \\ -\frac{1}{2\sigma^2}}$$
    and 
    $$A(\eta) = \frac{-\eta_1^2}{4\eta_2} + \frac{1}{2}\ln \eta_2 \qquad T(x) = \mat{x \\ x^2} \qquad \theta = \mat{\mu \\ \sigma^2} = \mat{\frac{-\eta_1}{2\eta_2} \\ \frac{-1}{2\eta_2}}$$
    
    \framebreak
    
    Now we use the previous conclusion to get the maximum likelihood estimation.
    
    $$\frac{dA(\eta)}{d \eta} = \frac{\sum_{i=1}^N T(x_i)}{N}$$
    $$A(\eta) = \frac{-\eta_1^2}{4\eta_2} + \frac{1}{2}\ln \eta_2$$
    $$\mat{-\frac{\eta_1}{2\eta_2} \\ \frac{\eta_1^2}{4\eta_2^2} + \frac{1}{2\eta_2}} = \mat{\frac{\sum_{i=1}^N x_i}{N} \\ \frac{\sum_{i=1}^N x_i^2}{N}}$$
    With 
    $$\theta = \mat{\mu \\ \sigma^2} = \mat{\frac{-\eta_1}{2\eta_2} \\ \frac{-1}{2\eta_2}}$$
    it is easy to solve $\mu$ and $\sigma^2$.
    Moreover, conjugate distribution in exponential familiy distribution are easy to compute.
    
    \framebreak
    
    We now show one important property of exponential family distribution that, for $$p(x | \lambda) = h(x) \exp (T(x)^\top \lambda - A_g(\lambda))$$ We have $$\nabla_\lambda A_g(\lambda) = \mathbb{E}_{p(x | \lambda)}[T(x)]$$
    From 
    $$\int_x p(x | \lambda) dx = \int_x h(x) \exp (T(x)^\top \lambda - A_g(\lambda))dx = 1$$
    \begin{gather*}
    \nabla_\lambda \int_x h(x) \exp (T(x)^\top \lambda - A_g(\lambda))dx = 0 \\
    \int_x \nabla_\lambda h(x) \exp (T(x)^\top \lambda - A_g(\lambda))dx = 0 \\
    \int_x \nabla_\lambda h(x) \exp (T(x)^\top \lambda - A_g(\lambda))dx = 0 \\
    \int_x p(x | \lambda)(T(x) - \nabla A_g(\lambda))dx = 0 \\
    \nabla_\lambda A_g(\lambda) = \mathbb{E}_{p(x | \lambda)}[T(x)]
    \end{gather*}
    Thus the equation holds.
    
    \end{frame} 